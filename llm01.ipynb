{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention\n",
    "# Query, Key, Value를 이용하여 Attention Score를 계산\n",
    "# Attention Score를 통해 Weighted Sum을 구함.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    # Q, K, V: Query, Key, Value 텐서 (입력 벡터)\n",
    "    # d_k: Query/Key 벡터의 차원 수\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Attention Score 계산: Q와 K의 전치행렬을 내적(dot product)하여 계산\n",
    "    # d_k의 제곱근으로 나누어 스코어의 크기를 조절\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(d_k)\n",
    "    \n",
    "    # Attention Score를 소프트맥스 함수로 정규화하여 가중치 계산\n",
    "    # d: 임베딩 차원\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 가중치를 이용해 V 벡터의 가중합을 계산하여 최종 출력값 생성\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Foward Neural Network (FFN): 각 단어의 임베딩 벡터를 비선형적으로 변환하는 두 개의 fc layer를 가짐\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# FFN 클래스 정의\n",
    "class FeedFowardNN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 첫 번째 선형 변환층: 입력 차원(d_model)을 중간 차원(d_ff)으로 변환\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # 비선형 활성화 함수 ReLU 정의\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 두 번째 선형 변환층: 중간 차원(d_ff)을 출력 차원(d_model)으로 변환\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    # 순전파 함수 정의\n",
    "    def forward(self, x):\n",
    "        # 입력 x에 대해 첫 번째 선형 변환 후 ReLU 활성화 함수 적용,\n",
    "        # 두 번째 선형 변환을 통해 출력 반환\n",
    "        return self.linear2(self.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization: 각 레이어의 출력을 정규화함.\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Layer Normalization 클래스 정의\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 학습 가능한 파라미터 scale(스케일링)과 shift(이동) 초기화\n",
    "        # d_model 크기의 1로 초기화된 파라미터 (scale)\n",
    "        # [1, 1, ... , 1]\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "        \n",
    "        # d_model 크기의 0으로 초기화된 파라미터 (shift)\n",
    "        # [0, 0, ... , 0]\n",
    "        self.shift = nn.Parameter(torch.zeros(d_model))\n",
    "        \n",
    "        # 작은 값을 더하여 분모가 0이 되는 것을 방지하는 epsilon 설정\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 각 마지막 차원(-1)에 대해 평균 계산\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        \n",
    "        # 각 마지막 차원(-1)에 대해 표준편차 계산\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        \n",
    "        # 입력 x를 정규화하고 scale과 shift를 적용하여 반환\n",
    "        return self.scale * (x - mean) / (std + self.eps) + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Connection: 입력에 출력을 더하는 연산\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# 잔차 연결(Residual Connection) 클래스 정의\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer Normalization 초기화\n",
    "        self.layer_norm = LayerNorm(size)\n",
    "\n",
    "    # 순전파 함수 정의\n",
    "    def forward(self, x, sublayer):\n",
    "        # 입력 x에 대해 Layer Normalization을 수행한 결과를 sublayer에 전달\n",
    "        # sublayer의 출력을 원래 입력 x에 더해 반환 (잔차 연결)\n",
    "        return x + sublayer(self.layer_norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
